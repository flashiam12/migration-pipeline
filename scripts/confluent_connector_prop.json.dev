{
    "MySqlCdcSourceV2" : [
        {
            "config": "connector.class",
            "documentation": "Connector class",
            "is_required": true
            },
            {
            "config": "database.hostname",
            "documentation": "IP address or hostname of the MySQL database server.",
            "is_required": true
            },
            {
            "config": "database.password",
            "documentation": "Password for the MySQL database user that has the required authorization.",
            "is_required": true
            },
            {
            "config": "database.port",
            "documentation": "Port number of the MySQL database server.",
            "is_required": true
            },
            {
            "config": "database.user",
            "documentation": "The name of the MySQL database user that has the required authorization.",
            "is_required": true
            },
            {
            "config": "name",
            "documentation": "Sets a name for your connector.",
            "is_required": true
            },
            {
            "config": "output.data.format",
            "documentation": "Sets the output Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF",
            "is_required": true
            },
            {
            "config": "tasks.max",
            "documentation": "Maximum number of tasks for the connector.",
            "is_required": true
            },
            {
            "config": "topic.prefix",
            "documentation": "Topic prefix that provides a namespace (logical server name) for the particular MySQL database server or cluster in which Debezium is capturing changes. The prefix should be unique across all other connectors, since it is used as a topic name prefix for all Kafka topics that receive records from this connector. Only alphanumeric characters, hyphens, dots and underscores must be used. The connector automatically creates Kafka topics using the naming convention: `\u003ctopic.prefix\u003e.\u003cdatabaseName\u003e.\u003ctableName\u003e`.",
            "is_required": true
            },
            {
            "config": "after.state.only",
            "documentation": "Controls whether the generated Kafka record should contain only the state of the row after the event occurred.",
            "is_required": false
            },
            {
            "config": "auto.restart.on.user.error",
            "documentation": "Enable connector to automatically restart on user-actionable errors.",
            "is_required": false
            },
            {
            "config": "column.exclude.list",
            "documentation": "An optional, comma-separated list of regular expressions that match the fully-qualified names of columns to exclude from change event record values. Fully-qualified names for columns are of the form `databaseName.tableName.columnName`. \nTo match the name of a column, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the column; it does not match substrings that might be present in a column name.",
            "is_required": false
            },
            {
            "config": "csfle.enabled",
            "documentation": "Determines whether the connector honours CSFLE rules or not",
            "is_required": false
            },
            {
            "config": "database.include.list",
            "documentation": "An optional, comma-separated list of regular expressions that match the names of the databases for which to capture changes. The connector does not capture changes in any database whose name is not in this list. By default, the connector captures changes in all databases.\nTo match the name of a database, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire name string of the database; it does not match substrings that might be present in a database name.",
            "is_required": false
            },
            {
            "config": "database.ssl.mode",
            "documentation": "Whether to use an encrypted connection to the MySQL server. Possible settings are: `disabled`, `preferred`, and `required`. \n`disabled` specifies the use of an unencrypted connection. \n`preferred` establishes an encrypted connection if the server supports secure connections. If the server does not support secure connections, falls back to an unencrypted connection. \n`required` establishes an encrypted connection or fails if one cannot be made for any reason.",
            "is_required": false
            },
            {
            "config": "datapreview.schemas.enable",
            "documentation": "This config key only applies to data preview requests and governs whether the data preview output has record schema with it.\nThe visibility condition is set such that it can never be true.\nSo this key does not show in create connector UI.",
            "is_required": false
            },
            {
            "config": "decimal.handling.mode",
            "documentation": "Specifies how the connector should handle values for `DECIMAL` and `NUMERIC` columns. Possible settings are: `precise`, `double`, and `string`. \n`precise` represents values by using `java.math.BigDecimal` to represent values in binary form in change events. `double` represents values by using double values, which might result in a loss of precision but which is easier to use. `string` encodes values as formatted strings, which are easy to consume but semantic information about the real type is lost.",
            "is_required": false
            },
            {
            "config": "event.processing.failure.handling.mode",
            "documentation": "Specifies how the connector should react to exceptions during processing of events. Possible settings are: `fail`, `skip`, and `warn`. \n`fail` propagates the exception, indicates the offset of the problematic event, and causes the connector to stop. \n`warn` logs the offset of the problematic event, skips that event, and continues processing. \n`skip` skips the problematic event and continues processing.",
            "is_required": false
            },
            {
            "config": "field.name.adjustment.mode",
            "documentation": "Specifies how field names should be adjusted for compatibility with the message converter used by the connector. Possible settings are: `none`, `avro`, and `avro_unicode`. \n`none` does not apply any adjustment. \n`avro` replaces the characters that cannot be used in the Avro type name with underscore. \n`avro_unicode` replaces the underscore or characters that cannot be used in the Avro type name with corresponding unicode like _uxxxx. Note: _ is an escape sequence like backslash in Java.",
            "is_required": false
            },
            {
            "config": "heartbeat.interval.ms",
            "documentation": "Controls how frequently the connector sends heartbeat messages to a Kafka topic. The behavior of default value 0 is that the connector does not send heartbeat messages. Heartbeat messages are useful for monitoring whether the connector is receiving change events from the database. Heartbeat messages might help decrease the number of change events that need to be re-sent when a connector restarts. To send heartbeat messages, set this property to a positive integer, which indicates the number of milliseconds between heartbeat messages.",
            "is_required": false
            },
            {
            "config": "ignore.default.for.nullables",
            "documentation": "When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value.",
            "is_required": false
            },
            {
            "config": "inconsistent.schema.handling.mode",
            "documentation": "Specifies how the connector should react to binlog events that belong to a table missing from internal schema representation. Possible settings are: `fail`, `skip`, and `warn`. \n`fail` - throws an exception that indicates the problematic event and its binlog offset, and causes the connector to stop. \n`skip` - passes over the problematic event and does not log anything. \n`warn` - logs the problematic event and its binlog offset and skips the event.",
            "is_required": false
            },
            {
            "config": "json.output.decimal.format",
            "documentation": "Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals:\nBASE64 to serialize DECIMAL logical types as base64 encoded binary data and\nNUMERIC to serialize Connect DECIMAL logical type values in JSON/JSON_SR as a number representing the decimal value.",
            "is_required": false
            },
            {
            "config": "kafka.api.key",
            "documentation": "Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.",
            "is_required": false
            },
            {
            "config": "kafka.api.secret",
            "documentation": "Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.",
            "is_required": false
            },
            {
            "config": "kafka.auth.mode",
            "documentation": "Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.",
            "is_required": false
            },
            {
            "config": "kafka.service.account.id",
            "documentation": "The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.",
            "is_required": false
            },
            {
            "config": "key.converter.reference.subject.name.strategy",
            "documentation": "Set the subject reference name strategy for key. Valid entries are `DefaultReferenceSubjectNameStrategy` or `QualifiedReferenceSubjectNameStrategy`. Note that the subject reference name strategy can be selected only for `PROTOBUF` format with the default strategy being `DefaultReferenceSubjectNameStrategy`.",
            "is_required": false
            },
            {
            "config": "output.key.format",
            "documentation": "Sets the output Kafka record key format. Valid entries are AVRO, JSON_SR, PROTOBUF, STRING or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF",
            "is_required": false
            },
            {
            "config": "predicates",
            "documentation": "Aliases for the predicates used by transformations.",
            "is_required": false
            },
            {
            "config": "replace.null.with.default",
            "documentation": "Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used.",
            "is_required": false
            },
            {
            "config": "schema.context.name",
            "documentation": "Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.",
            "is_required": false
            },
            {
            "config": "schema.history.internal.kafka.topic",
            "documentation": "The name of the topic for the database schema history. A new topic with provided name is created, if it doesn't already exist. If the topic already exists, ensure that it has a single partition, infinite retention period and is not in use by any other connector. If no value is provided, the name defaults to ``dbhistory.\u003ctopic-prefix\u003e.\u003clcc-id\u003e``.",
            "is_required": false
            },
            {
            "config": "schema.history.internal.skip.unparseable.ddl",
            "documentation": "A Boolean value that specifies whether the connector should ignore malformed or unknown database statements (`true`), or stop processing so a human can fix the issue (`false`). Defaults to `false`. Consider setting this to `true` to ignore unparseable statements.",
            "is_required": false
            },
            {
            "config": "schema.history.internal.store.only.captured.tables.ddl",
            "documentation": "A Boolean value that specifies whether the connector records schema structures from all tables in a schema or database, or only from tables that are designated for capture. Defaults to `false`. \n`false` - During a database snapshot, the connector records the schema data for all non-system tables in the database, including tables that are not designated for capture. It’s best to retain the default setting. If you later decide to capture changes from tables that you did not originally designate for capture, the connector can easily begin to capture data from those tables, because their schema structure is already stored in the schema history topic. \n`true` - During a database snapshot, the connector records the table schemas only for the tables from which Debezium captures change events. If you change the default value, and you later configure the connector to capture data from other tables in the database, the connector lacks the schema information that it requires to capture change events from the tables.",
            "is_required": false
            },
            {
            "config": "schema.name.adjustment.mode",
            "documentation": "Specifies how schema names should be adjusted for compatibility with the message converter used by the connector. Possible settings are: `none`, `avro`, and `avro_unicode`. \n`none` does not apply any adjustment. \n`avro` replaces the characters that cannot be used in the Avro type name with underscore. \n`avro_unicode` replaces the underscore or characters that cannot be used in the Avro type name with corresponding unicode like _uxxxx. Note: _ is an escape sequence like backslash in Java.",
            "is_required": false
            },
            {
            "config": "schemas.enable",
            "documentation": "Include schemas within each of the serialized values.",
            "is_required": false
            },
            {
            "config": "signal.data.collection",
            "documentation": "Fully-qualified name of the data collection that needs to be used to send signals to the connector. Use the following format to specify the fully-qualified collection name: `databaseName.tableName` ",
            "is_required": false
            },
            {
            "config": "snapshot.locking.mode",
            "documentation": "Controls whether and how long the connector holds the global MySQL read lock, which prevents any updates to the database, while the connector is performing a snapshot. Possible settings are: `minimal`, `minimal_percona`, `extended`, and `none`. \n`minimal` - the connector holds the global read lock for just the initial portion of the snapshot, while the database schemas and other metadata are being read. The remaining work in a snapshot involves selecting all rows from each table. This is accomplished using a REPEATABLE READ transaction, even when the lock is no longer held and other MySQL clients are updating the database. \n`minimal_percona` - similar to `minimal` mode except the connector uses a (Percona-specific) backup lock. This mode does not flush tables to disk, is not blocked by long-running reads, and is available only in Percona Server. \n`extended` - blocks all writes for the duration of the snapshot. Use this setting if there are clients that are submitting operations that MySQL excludes from REPEATABLE READ semantics. \n`none` - prevents the connector from acquiring any table locks during the snapshot. While this setting is allowed with all snapshot modes, it is safe to use if and only if no schema changes are happening while the snapshot is running. For tables defined with MyISAM engine, the tables would still be locked despite this property being set as MyISAM acquires a table lock. This behavior is unlike InnoDB engine, which acquires row level locks.",
            "is_required": false
            },
            {
            "config": "snapshot.mode",
            "documentation": "Specifies the criteria for running a snapshot when the connector starts. Possible settings are: `initial`, `initial_only`, `when_needed`, `never`, `schema_only`, `schema_only_recovery`. \n`initial` - the connector runs a snapshot only when no offsets have been recorded for the logical server name. \n`initial_only` - the connector runs a snapshot only when no offsets have been recorded for the logical server name and then stops; i.e. it will not read change events from the binlog. \n`when_needed` - the connector runs a snapshot upon startup whenever it deems it necessary. That is, when no offsets are available, or when a previously recorded offset specifies a binlog location or GTID that is not available in the server. \n`never` - the connector never uses snapshots. Upon first startup with a logical server name, the connector reads from the beginning of the binlog. Configure this behavior with care. It is valid only when the binlog is guaranteed to contain the entire history of the database. \n`schema_only` - the connector runs a snapshot of the schemas and not the data. This setting is useful when you do not need the topics to contain a consistent snapshot of the data but need them to have only the changes since the connector was started. \n`schema_only_recovery` - this is a recovery setting for a connector that has already been capturing changes. When you restart the connector, this setting enables recovery of a corrupted or lost database schema history topic. You might set it periodically to \"clean up\" a database schema history topic that has been growing unexpectedly. Database schema history topics require infinite retention.",
            "is_required": false
            },
            {
            "config": "sr.service.account.id",
            "documentation": "Select the service account that has appropriate permissions to schemas and encryption keys in the Schema Registry.",
            "is_required": false
            },
            {
            "config": "table.exclude.list",
            "documentation": "An optional, comma-separated list of regular expressions that match fully-qualified table identifiers for tables whose changes you do not want to capture. Each identifier is of the form `database.tableName`. When this property is set, the connector captures changes from every table that you do not specify. \nTo match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire identifier for the table; it does not match substrings that might be present in a table name. \nIf you include this property in the configuration, do not set the ``table.include.list`` property.",
            "is_required": false
            },
            {
            "config": "table.include.list",
            "documentation": "An optional, comma-separated list of regular expressions that match fully-qualified table identifiers for tables whose changes you want to capture. When this property is set, the connector captures changes only from the specified tables. Each identifier is of the form `database.tableName`. By default, the connector captures changes in every non-system table in each schema whose changes are being captured. \nTo match the name of a table, Debezium applies the regular expression that you specify as an anchored regular expression. That is, the specified expression is matched against the entire identifier for the table; it does not match substrings that might be present in a table name. \nIf you include this property in the configuration, do not also set the ``table.exclude.list`` property.",
            "is_required": false
            },
            {
            "config": "time.precision.mode",
            "documentation": "Time, date, and timestamps can be represented with different kinds of precisions: \n`adaptive_time_microseconds` captures the date, datetime and timestamp values exactly as in the database using either millisecond, microsecond, or nanosecond precision values based on the database column’s type. An exception is `TIME` type fields, which are always captured as microseconds. \n`connect` always represents time and timestamp values by using Kafka Connect’s built-in representations for `Time`, `Date`, and `Timestamp`, which use millisecond precision regardless of the database columns' precision.",
            "is_required": false
            },
            {
            "config": "tombstones.on.delete",
            "documentation": "Controls whether a tombstone event should be generated after a delete event. \n`true` - a delete operation is represented by a delete event and a subsequent tombstone event. \nfalse - only a delete event is emitted. \nAfter a source record is deleted, emitting the tombstone event (the default behavior) allows Kafka to completely delete all events that pertain to the key of the deleted row in case log compaction is enabled for the topic.",
            "is_required": false
            },
            {
            "config": "transforms",
            "documentation": "Aliases for the transformations to be applied to records.",
            "is_required": false
            },
            {
            "config": "value.converter.ignore.default.for.nullables",
            "documentation": "Specifies whether the default value should be ignored for nullable fields. If set to `true`, the default value is ignored for nullable fields. If set to `false`, the default value is used for nullable fields.",
            "is_required": false
            },
            {
            "config": "value.converter.reference.subject.name.strategy",
            "documentation": "Set the subject reference name strategy for value. Valid entries are `DefaultReferenceSubjectNameStrategy` or `QualifiedReferenceSubjectNameStrategy`. Note that the subject reference name strategy can be selected only for `PROTOBUF` format with the default strategy being `DefaultReferenceSubjectNameStrategy`.",
            "is_required": false
            },
            {
            "config": "value.converter.replace.null.with.default",
            "documentation": "Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used.",
            "is_required": false
            },
            {
            "config": "value.converter.schemas.enable",
            "documentation": "Include schemas within each of the serialized values.",
            "is_required": false
            }
    ],
    "DataScienceBigQueryStorageSink": [
        {
          "config": "connector.class",
          "documentation": "Connector class",
          "is_required": true
        },
        {
          "config": "datasets",
          "documentation": "Name of the BigQuery dataset where table(s) is located.",
          "is_required": true
        },
        {
          "config": "ingestion.mode",
          "documentation": "Select a mode to ingest data into the table. Select STREAMING for reduced latency. Select BATCH LOADING for cost savings. Select UPSERT for upserting records. Select UPSERT_DELETE for upserting and deleting records.",
          "is_required": true
        },
        {
          "config": "name",
          "documentation": "Sets a name for your connector.",
          "is_required": true
        },
        {
          "config": "project",
          "documentation": "ID for the GCP project where BigQuery is located.",
          "is_required": true
        },
        {
          "config": "tasks.max",
          "documentation": "Maximum number of tasks for the connector.",
          "is_required": true
        },
        {
          "config": "authentication.method",
          "documentation": "Select how you want to authenticate with BigQuery.",
          "is_required": false
        },
        {
          "config": "auto.create.tables",
          "documentation": "Designates whether or not to automatically create BigQuery tables. Note: Supports AVRO, JSON_SR, and PROTOBUF message format only.",
          "is_required": false
        },
        {
          "config": "auto.restart.on.user.error",
          "documentation": "Enable connector to automatically restart on user-actionable errors.",
          "is_required": false
        },
        {
          "config": "auto.update.schemas",
          "documentation": "Designates whether or not to automatically update BigQuery schemas. New fields in record schemas must be nullable. Note: Supports AVRO, JSON_SR, and PROTOBUF message format only.",
          "is_required": false
        },
        {
          "config": "cloudevent.data.class",
          "documentation": "Data class for CloudEvent",
          "is_required": false
        },
        {
          "config": "column.names",
          "documentation": "List of Column Names",
          "is_required": false
        },
        {
          "config": "commit.interval",
          "documentation": "The interval, in seconds, the connector attempts to commit streamed records. Set the interval between 60 seconds (1 minute) and 14,400 seconds (4 hours). Be careful when setting the commit interval as on every commit interval, a task calls the ``CreateWriteStream`` API which is subject to a `quota \u003chttps://cloud.google.com/bigquery/quotas#write-api-limits:~:text=handle%20unexpected%20demand.-,CreateWriteStream,-requests\u003e`. For example, if you have five tasks (may belong to different connectors also) set to commit every 60 seconds to a project, there will be five calls to ``CreateWriteStream`` API on that project every minute. If the count exceeds the allowed quota, some tasks may fail.",
          "is_required": false
        },
        {
          "config": "convert.double.special.values",
          "documentation": "Convert Double Special Values",
          "is_required": false
        },
        {
          "config": "input.data.format",
          "documentation": "Sets the input Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, OPENX_METRICS_PROTOBUF, OPENCENSUS_PROTOBUF, OTEL_PROTOBUF, OTEL_TRACE_PROTOBUF, CLOUDEVENT_PROTOBUF, JSON or OTEL_LOGS_PROTOBUF. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, or PROTOBUF.",
          "is_required": false
        },
        {
          "config": "input.key.format",
          "documentation": "Sets the input Kafka record key format. Valid entries are AVRO, BYTES, JSON, JSON_SR, PROTOBUF. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF",
          "is_required": false
        },
        {
          "config": "kafka.api.key",
          "documentation": "Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.",
          "is_required": false
        },
        {
          "config": "kafka.api.secret",
          "documentation": "Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.",
          "is_required": false
        },
        {
          "config": "kafka.auth.mode",
          "documentation": "Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.",
          "is_required": false
        },
        {
          "config": "kafka.service.account.id",
          "documentation": "The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.",
          "is_required": false
        },
        {
          "config": "keyfile",
          "documentation": "GCP service account JSON file with write permissions for BigQuery.",
          "is_required": false
        },
        {
          "config": "log.attribute.prefix",
          "documentation": "Prefix for All Log Attributes",
          "is_required": false
        },
        {
          "config": "max.poll.interval.ms",
          "documentation": "The maximum delay between subsequent consume requests to Kafka. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 300000 milliseconds (5 minutes).",
          "is_required": false
        },
        {
          "config": "max.poll.records",
          "documentation": "The maximum number of records to consume from Kafka in a single request. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 500 records.",
          "is_required": false
        },
        {
          "config": "metric.attribute.prefix",
          "documentation": "Prefix for Metric Attributes",
          "is_required": false
        },
        {
          "config": "metric.dimension",
          "documentation": "Metric Dimension",
          "is_required": false
        },
        {
          "config": "metric.label.prefix",
          "documentation": "Prefix for All Metrics",
          "is_required": false
        },
        {
          "config": "oauth.client.id",
          "documentation": "Client ID of your Google OAuth application.",
          "is_required": false
        },
        {
          "config": "oauth.client.secret",
          "documentation": "Client secret of your Google OAuth application.",
          "is_required": false
        },
        {
          "config": "oauth.refresh.token",
          "documentation": "OAuth 2.0 refresh token for BigQuery.",
          "is_required": false
        },
        {
          "config": "partitioning.type",
          "documentation": "The time partitioning type to use when creating new partitioned tables. Existing tables will not be altered to use this partitioning type.",
          "is_required": false
        },
        {
          "config": "predicates",
          "documentation": "Aliases for the predicates used by transformations.",
          "is_required": false
        },
        {
          "config": "resource.attribute.prefix",
          "documentation": "Prefix for Resource Attributes",
          "is_required": false
        },
        {
          "config": "resource.label.prefix",
          "documentation": "Prefix for Resource Metrics",
          "is_required": false
        },
        {
          "config": "sanitize.field.names",
          "documentation": "Whether to automatically sanitize field names before using them as field names in BigQuery. BigQuery specifies that field names can only contain letters, numbers, and underscores. The sanitizer replaces invalid symbols with underscores. If the field name starts with a digit, the sanitizer adds an underscore in front of field name. Caution: Key duplication errors can occur if different fields are named a.b and a_b, for instance. After being sanitized, field names a.b and a_b will have same value.",
          "is_required": false
        },
        {
          "config": "sanitize.topics",
          "documentation": "Designates whether to automatically sanitize topic names before using them as table names in BigQuery. If not enabled, topic names are used as table names.",
          "is_required": false
        },
        {
          "config": "schema.context.name",
          "documentation": "Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.",
          "is_required": false
        },
        {
          "config": "span.attribute.prefix",
          "documentation": "Prefix for All Span Attributes",
          "is_required": false
        },
        {
          "config": "timestamp.partition.field.name",
          "documentation": "The name of the field in the value that contains the timestamp to partition by in BigQuery. This also enables timestamp partitioning for each table.",
          "is_required": false
        },
        {
          "config": "topic2table.map",
          "documentation": "Map of topics to tables (optional). The required format is comma-separated tuples. For example, \u003ctopic-1\u003e:\u003ctable-1\u003e,\u003ctopic-2\u003e:\u003ctable-2\u003e,... Note that a topic name must not be modified using a regex SMT while using this option. Note that if this property is used, ``sanitize.topics`` is ignored. Also, if the topic-to-table map doesn’t contain the topic for a record, the connector creates a table with the same name as the topic name.",
          "is_required": false
        },
        {
          "config": "topics",
          "documentation": "Identifies the topic name or a comma-separated list of topic names.",
          "is_required": false
        },
        {
          "config": "transforms",
          "documentation": "Type of Transformation",
          "is_required": false
        },
        {
          "config": "transforms",
          "documentation": "Aliases for the transformations to be applied to records.",
          "is_required": false
        },
        {
          "config": "use.date.time.formatter",
          "documentation": "Specify whether to use a ``DateTimeFormatter`` to support a wide range of epochs. Setting this true will use ``DateTimeFormatter`` over default ``SimpleDateFormat``. The output might vary for same input between the two formatters.",
          "is_required": false
        },
        {
          "config": "value.converter.reference.subject.name.strategy",
          "documentation": "Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.",
          "is_required": false
        }
    ]
}